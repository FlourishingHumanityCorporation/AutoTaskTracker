{
  "original": {
    "test_bulk_operation_opportunities": {
      "status": "PASS",
      "stdout": "\n\ud83d\udca1 BULK OPERATION OPPORTUNITIES \ud83d\udca1\n\nFound 6 files with potential bulk operation improvements:\n\n  \ud83d\udca1 autotasktracker/pensieve/postgresql_adapter.py\n  \ud83d\udca1 autotasktracker/pensieve/vector_search.py\n  \ud83d\udca1 autotasktracker/pensieve/advanced_search.py\n  \ud83d\udca1 autotasktracker/pensieve/api_client.py\n  \ud83d\udca1 autotasktracker/core/database.py\n  \ud83d\udca1 autotasktracker/dashboards/realtime_dashboard.py\n\n\nConsider using bulk operations for better performance:\n\nInstead of:\n  for item in items:\n      db.store_metadata(entity_id, key, item)\n\nUse:\n  db.store_metadata_batch([(entity_id, key, item) for item in items])\n\n",
      "stderr": "",
      "error": null,
      "issue_counts": {
        "bulk_operations": 6
      }
    },
    "test_cache_management": {
      "status": "PASS",
      "stdout": "\n\u26a0\ufe0f CACHE MANAGEMENT WARNING \u26a0\ufe0f\n\nFound 2 files using cache without cleanup logic:\n\n  \u26a0\ufe0f autotasktracker/config.py\n  \u26a0\ufe0f autotasktracker/core/config_manager.py\n\nConsider implementing cache management:\n- Monitor cache size\n- Implement cleanup for old files\n- Set cache size limits\n- Add cache expiration\n\nExample:\n  def cleanup_cache(cache_dir, max_size_gb=10, max_age_days=30):\n      # Remove old files and limit cache size\n\n",
      "stderr": "",
      "error": null,
      "issue_counts": {
        "cache_management": 2
      }
    },
    "test_configuration_hardcoding": {
      "status": "PASS",
      "stdout": "\n\u26a0\ufe0f HARDCODED VALUES DETECTED \u26a0\ufe0f\n\nFound 42 hardcoded values that should be configurable:\n\n\n\u26a0\ufe0f autotasktracker/pensieve/event_processor.py:123\n   Type: hardcoded timeout\n   Value: timeout=5\n   Context: self.processor_thread.join(timeout=5)...\n\n\n\u26a0\ufe0f autotasktracker/pensieve/event_processor.py:185\n   Type: hardcoded timeout\n   Value: timeout=1\n   Context: timeout=1,...\n\n\n\u26a0\ufe0f autotasktracker/pensieve/config_reader.py:63\n   Type: hardcoded timeout\n   Value: timeout=10\n   Context: timeout=10...\n\n\n\u26a0\ufe0f autotasktracker/pensieve/config_reader.py:283\n   Type: hardcoded URL\n   Value: \"http://{get_config().SERVER_HOST}:{pensieve_config.api_port}\"\n   Context: \"PENSIEVE_API_URL\": f\"http://{get_config().SERVER_HOST}:{pensieve_config.api_por...\n\n\n\u26a0\ufe0f autotasktracker/pensieve/config_reader.py:284\n   Type: hardcoded URL\n   Value: \"http://{get_config().SERVER_HOST}:{pensieve_config.web_port}\"\n   Context: \"PENSIEVE_WEB_URL\": f\"http://{get_config().SERVER_HOST}:{pensieve_config.web_por...\n\n\n\u26a0\ufe0f autotasktracker/pensieve/cache_manager.py:379\n   Type: hardcoded sleep\n   Value: sleep(300\n   Context: time.sleep(300)...\n\n\n\u26a0\ufe0f autotasktracker/pensieve/event_integration.py:244\n   Type: hardcoded timeout\n   Value: timeout=5\n   Context: requests.post(webhook_url, json=notification, timeout=5)...\n\n\n\u26a0\ufe0f autotasktracker/pensieve/event_integration.py:380\n   Type: hardcoded sleep\n   Value: sleep(5\n   Context: await asyncio.sleep(5)...\n\n\n\u26a0\ufe0f autotasktracker/pensieve/health_monitor.py:45\n   Type: hardcoded timeout\n   Value: timeout = 5\n   Context: self.api_timeout = 5...\n\n\n\u26a0\ufe0f autotasktracker/pensieve/health_monitor.py:210\n   Type: hardcoded timeout\n   Value: timeout=5\n   Context: self._monitor_thread.join(timeout=5)...\n\n\n\u26a0\ufe0f autotasktracker/pensieve/config_sync.py:26\n   Type: hardcoded port\n   Value: :8839\"\n   Context: api_base_url: str = \"http://localhost:8839\"...\n\n\n\u26a0\ufe0f autotasktracker/pensieve/config_sync.py:210\n   Type: hardcoded port\n   Value: :8839\"\n   Context: api_base_url=\"http://localhost:8839\"...\n\n\n\u26a0\ufe0f autotasktracker/pensieve/config_sync.py:142\n   Type: hardcoded URL\n   Value: \"http://{api_host}:{api_port}\"\n   Context: api_base_url = f\"http://{api_host}:{api_port}\"...\n\n\n\u26a0\ufe0f autotasktracker/pensieve/webhook_server.py:442\n   Type: hardcoded port\n   Value: :8840\"\n   Context: print(f\"Webhook server started on http://127.0.0.1:8840\")...\n\n\n\u26a0\ufe0f autotasktracker/pensieve/api_client.py:127\n   Type: hardcoded absolute path\n   Value: \"/api/libraries/{library_id}/folders/{folder_id}/entities\"\n   Context: endpoint=f\"/api/libraries/{library_id}/folders/{folder_id}/entities\"...\n\n... and 27 more\n\n\u2705 MAKE VALUES CONFIGURABLE:\n\n1. Move to configuration:\n   # config.py\n   VLM_TIMEOUT = int(os.getenv('VLM_TIMEOUT', '30'))\n   RETRY_ATTEMPTS = int(os.getenv('RETRY_ATTEMPTS', '3'))\n   BATCH_SIZE = int(os.getenv('BATCH_SIZE', '1000'))\n\n2. Use configuration:\n   from autotasktracker.config import get_config\n   config = get_config()\n   timeout = config.VLM_TIMEOUT\n\n3. For URLs:\n   base_url = config.API_BASE_URL\n   endpoint = f\"{base_url}/api/v1/process\"\n\nThis improves maintainability and deployment flexibility!\n\n",
      "stderr": "",
      "error": null,
      "issue_counts": {
        "hardcoded_values": 42
      }
    },
    "test_connection_pool_usage": {
      "status": "PASS",
      "stdout": "\n\u26a0\ufe0f CONNECTION POOLING ISSUES \u26a0\ufe0f\n\nFound 2 potential connection pooling issues:\n\n\n\u26a0\ufe0f autotasktracker/pensieve/backend_optimizer.py - __init__()\n   Issue: Multiple DatabaseManager instances in function (3 instances)\n\n\n\u26a0\ufe0f autotasktracker/pensieve/enhanced_search.py - __init__()\n   Issue: Multiple DatabaseManager instances in function (3 instances)\n\n\n\u2705 BEST PRACTICES:\n\n1. Reuse DatabaseManager instances:\n   def __init__(self):\n       self.db = DatabaseManager()  # Create once\n   \n   def method1(self):\n       data = self.db.fetch_tasks()  # Reuse\n   \n   def method2(self):\n       data = self.db.fetch_tasks()  # Reuse same instance\n\n2. Use context managers for connections:\n   with self.db.get_connection() as conn:\n       # Connection automatically returned to pool\n\n3. For scripts, create once at module level:\n   db = DatabaseManager()  # Module level\n   \n   def main():\n       data = db.fetch_tasks()  # Use module instance\n\n",
      "stderr": "",
      "error": null,
      "issue_counts": {
        "connection_pooling": 2
      }
    },
    "test_error_handling_patterns": {
      "status": "FAIL",
      "stdout": "Processing files: 20/50\nProcessing files: 40/50\nProcessing files: 50/50\n",
      "stderr": "",
      "error": "\n\ud83d\udea8 POOR ERROR HANDLING IN PENSIEVE INTEGRATION \ud83d\udea8\n\nFound 1 error handling issues in 1 files:\n\n\u274c autotasktracker/dashboards/base.py   Line 347: Silent Pass   Code: pass...\n\n\u2705 GOOD ERROR HANDLING:\n  try:\n      result = db.fetch_tasks()\n  except sqlite3.DatabaseError as e:\n      logger.error(f\"Database error: {{e}}\")\n      # Specific recovery action\n  except Exception as e:\n      logger.exception(\"Unexpected error\")\n      # Graceful degradation\n\n\u274c AVOID:\n  - Bare except clauses\n  - Printing errors instead of logging\n  - Silently passing exceptions\n  - Not handling specific error types\n\n\ud83d\udca1 TIP: Set PENSIEVE_AUTO_FIX=1 to automatically fix some issues\n",
      "issue_counts": {}
    },
    "test_file_operation_validation": {
      "status": "FAIL",
      "stdout": "",
      "stderr": "",
      "error": "\n\ud83d\udea8 FILE OPERATION VALIDATION MISSING \ud83d\udea8\n\nFound 3 file operations without validation:\n\n\u274c autotasktracker/pensieve/config_sync.py:236\n   Function: _save_config_to_file()\n   Operation: file open\n   Code: open(self.config_file\n\n\n\u274c autotasktracker/core/config_manager.py:234\n   Function: save_config_snapshot()\n   Operation: file open\n   Code: open(file_path\n\n\n\u274c autotasktracker/dashboards/launcher.py:126\n   Function: launch_dashboard()\n   Operation: file open\n   Code: open(\n                cmd\n\n\n\u2705 ALWAYS VALIDATE FILE OPERATIONS:\n\n# Before reading:\nif not os.path.exists(filepath):\n    logger.error(f\"File not found: {{filepath}}\")\n    return None\n\n# Check permissions:\nif not os.access(filepath, os.R_OK):\n    raise PermissionError(f\"Cannot read file: {{filepath}}\")\n\n# Use Path for better validation:\nfrom pathlib import Path\nfile_path = Path(filepath)\nif not file_path.is_file():\n    raise FileNotFoundError(f\"Not a file: {{filepath}}\")\n\n# Always use try/except:\ntry:\n    with open(filepath, 'r') as f:\n        content = f.read()\nexcept IOError as e:\n    logger.error(f\"Failed to read file: {{e}}\")\n    raise\n",
      "issue_counts": {}
    },
    "test_generate_summary_report": {
      "status": "PASS",
      "stdout": "\n============================================================\nPENSIEVE HEALTH TEST SUMMARY\n============================================================\nMode: full\nFiles Analyzed: 50\nAuto-Fix: Disabled\n============================================================\n",
      "stderr": "",
      "error": null,
      "issue_counts": {}
    },
    "test_memos_command_usage": {
      "status": "PASS",
      "stdout": "",
      "stderr": "",
      "error": null,
      "issue_counts": {}
    },
    "test_metadata_key_consistency": {
      "status": "PASS",
      "stdout": "\n\ud83d\udea8 INCONSISTENT METADATA KEY USAGE DETECTED \ud83d\udea8\n\nFound 9 inconsistent metadata key patterns:\n\n\n\u274c Using 'ocr_result' instead of 'ocr_result':\n   Files: backend_optimizer.py, api_client.py, event_processor.py\n   ... and 7 more files\n\n\u274c Using 'text' instead of 'ocr_result':\n   Files: vlm_monitor.py, advanced_analytics.py, base.py\n   ... and 3 more files\n\n\u274c Using 'active_window' instead of 'active_window':\n   Files: notifications.py, vlm_monitor.py, base.py\n   ... and 10 more files\n\n\u274c Using 'tasks' instead of 'tasks':\n   Files: visualizations.py, base.py, event_processor.py\n   ... and 7 more files\n\n\u274c Using 'extracted_tasks' instead of 'tasks':\n   Files: webhook_server.py, event_processor.py, realtime_dashboard.py\n   ... and 1 more files\n\n\u274c Using 'category' instead of 'category':\n   Files: notifications.py, visualizations.py, base.py\n   ... and 6 more files\n\n\u274c Using 'vlm_structured' instead of 'vlm_structured':\n   Files: pensieve_adapter.py, database.py\n\n\u274c Using 'window' instead of 'active_window':\n   Files: task_board.py\n\n\u274c Using 'vlm_description' instead of 'vlm_structured':\n   Files: vlm_monitor.py\n\n\ud83d\udca1 TIP: Set PENSIEVE_AUTO_FIX=1 to automatically fix metadata keys\n",
      "stderr": "",
      "error": null,
      "issue_counts": {
        "inconsistent_keys": 9
      }
    },
    "test_missing_index_usage": {
      "status": "PASS",
      "stdout": "",
      "stderr": "",
      "error": null,
      "issue_counts": {}
    },
    "test_n_plus_one_query_patterns": {
      "status": "PASS",
      "stdout": "Processing files: 20/50\nProcessing files: 40/50\nProcessing files: 50/50\n\n\u26a0\ufe0f POTENTIAL N+1 QUERY PATTERNS DETECTED \u26a0\ufe0f\n\nFound 122 potential N+1 patterns in production code:\n\n\u274c autotasktracker/pensieve/event_processor.py:138\n   Loop at line 129 contains database query\n   Code: events = self._get_new_events()...\n\n\u274c autotasktracker/pensieve/event_processor.py:198\n   Loop at line 191 contains database query\n   Code: event = PensieveEvent(...\n\n\u274c autotasktracker/pensieve/event_processor.py:199\n   Loop at line 191 contains database query\n   Code: event_type=event_data.get('type', 'unknown'),...\n\n\u274c autotasktracker/pensieve/event_processor.py:200\n   Loop at line 191 contains database query\n   Code: entity_id=event_data.get('entity_id', 0),...\n\n\u274c autotasktracker/pensieve/event_processor.py:201\n   Loop at line 191 contains database query\n   Code: timestamp=datetime.fromisoformat(event_data.get('timestamp', datetime.now().isof...\n\n\u274c autotasktracker/pensieve/event_processor.py:201\n   Loop at line 191 contains database query\n   Code: timestamp=datetime.fromisoformat(event_data.get('timestamp', datetime.now().isof...\n\n\u274c autotasktracker/pensieve/event_processor.py:202\n   Loop at line 191 contains database query\n   Code: data=event_data.get('data', {}),...\n\n\u274c autotasktracker/pensieve/event_processor.py:517\n   Loop at line 515 contains database query\n   Code: stats = processor.get_statistics()...\n\n\u274c autotasktracker/pensieve/postgresql_adapter.py:147\n   Loop at line 141 contains database query\n   Code: metadata = self.pensieve_client.get_metadata(frame.id)...\n\n\u274c autotasktracker/pensieve/postgresql_adapter.py:154\n   Loop at line 141 contains database query\n   Code: \"tasks\": self._parse_tasks_safely(metadata.get(\"tasks\")),...\n\n... and 112 more\n\n\ud83d\udca1 Consider using batch operations or JOIN queries for better performance\n",
      "stderr": "",
      "error": null,
      "issue_counts": {
        "n_plus_one": 122
      }
    },
    "test_no_direct_sqlite_access": {
      "status": "PASS",
      "stdout": "",
      "stderr": "",
      "error": null,
      "issue_counts": {}
    },
    "test_pensieve_service_checks": {
      "status": "PASS",
      "stdout": "\n\u26a0\ufe0f MISSING PENSIEVE SERVICE CHECKS \u26a0\ufe0f\n\nFound 15 critical files without service checks:\n\n  \u26a0\ufe0f autotasktracker/dashboards/achievement_board.py\n  \u26a0\ufe0f autotasktracker/dashboards/cache.py\n  \u26a0\ufe0f autotasktracker/dashboards/__init__.py\n  \u26a0\ufe0f autotasktracker/dashboards/templates.py\n  \u26a0\ufe0f autotasktracker/dashboards/advanced_analytics.py\n  \u26a0\ufe0f autotasktracker/dashboards/utils.py\n  \u26a0\ufe0f autotasktracker/dashboards/realtime_dashboard.py\n  \u26a0\ufe0f autotasktracker/dashboards/analytics.py\n  \u26a0\ufe0f autotasktracker/dashboards/components/metrics.py\n  \u26a0\ufe0f autotasktracker/dashboards/components/data_display.py\n  \u26a0\ufe0f autotasktracker/dashboards/components/__init__.py\n  \u26a0\ufe0f autotasktracker/dashboards/components/visualizations.py\n  \u26a0\ufe0f autotasktracker/dashboards/components/filters.py\n  \u26a0\ufe0f autotasktracker/dashboards/data/models.py\n  \u26a0\ufe0f autotasktracker/dashboards/data/__init__.py\n\nAdd service status checks:\n\ndef check_pensieve_status():\n    try:\n        db = DatabaseManager()\n        if not db.test_connection():\n            st.error(\"Cannot connect to Pensieve database\")\n            st.info(\"Run: memos start\")\n            return False\n        return True\n    except Exception as e:\n        logger.error(\"Pensieve check failed in example code\")\n        return False\n\n",
      "stderr": "",
      "error": null,
      "issue_counts": {
        "missing_checks": 15
      }
    },
    "test_rest_api_utilization": {
      "status": "PASS",
      "stdout": "\n\u2139\ufe0f REST API USAGE DETECTED (Progress!)\n\nFound REST API references in 3 files:\n  \u2705 autotasktracker/pensieve/config_reader.py\n  \u2705 autotasktracker/pensieve/config_sync.py\n  \u2705 autotasktracker/pensieve/api_client.py\n\nThis is good! The audit showed 0% REST API usage.\n\n",
      "stderr": "",
      "error": null,
      "issue_counts": {}
    },
    "test_retry_logic_implementation": {
      "status": "PASS",
      "stdout": "\n\u26a0\ufe0f MISSING RETRY LOGIC WARNING \u26a0\ufe0f\n\nFound 6 files with network operations but no retry logic:\n\n\n\u26a0\ufe0f autotasktracker/pensieve/postgresql_adapter.py\n   Reason: Session requests without retry logic\n\n\n\u26a0\ufe0f autotasktracker/pensieve/event_integration.py\n   Reason: HTTP requests without retry logic\n\n\n\u26a0\ufe0f autotasktracker/core/database.py\n   Reason: Network connections without retry logic\n\n\n\u26a0\ufe0f autotasktracker/core/error_handler.py\n   Reason: HTTP requests without retry logic\n\n\n\u26a0\ufe0f autotasktracker/core/error_handler.py\n   Reason: Ollama API calls without retry logic\n\n\n\u26a0\ufe0f autotasktracker/dashboards/vlm_monitor.py\n   Reason: Ollama API calls without retry logic\n\n\n\n\u2705 GOOD EXAMPLES found in 2 files\n\n\u2705 IMPLEMENT RETRY LOGIC:\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10)\n)\ndef api_call_with_retry():\n    response = requests.get(url)\n    response.raise_for_status()\n    return response\n\nOr manual implementation:\nfor attempt in range(max_retries):\n    try:\n        result = api_call()\n        break\n    except Exception as e:\n        if attempt == max_retries - 1:\n            raise\n        time.sleep(2 ** attempt)  # Exponential backoff\n\n",
      "stderr": "",
      "error": null,
      "issue_counts": {
        "retry_issues": 6
      }
    },
    "test_transaction_management": {
      "status": "PASS",
      "stdout": "\n\u26a0\ufe0f TRANSACTION MANAGEMENT WARNING \u26a0\ufe0f\n\nFound 1 functions with multiple write operations but no transactions:\n\n\n\u26a0\ufe0f autotasktracker/pensieve/postgresql_adapter.py - get_migration_recommendations()\n   Line 362: 2 write operations without transaction\n   First operation at line 383: UPDATE\n\n\n\n\u2705 RECOMMENDED:\n  with db.get_connection() as conn:\n      cursor = conn.cursor()\n      cursor.execute(\"BEGIN TRANSACTION\")\n      try:\n          # Multiple operations\n          db.store_metadata(entity_id, \"tasks\", task_data, conn=conn)\n          db.store_metadata(entity_id, \"category\", category, conn=conn)\n          cursor.execute(\"COMMIT\")\n      except:\n          cursor.execute(\"ROLLBACK\")\n          raise\n\nThis ensures atomicity and better performance.\n\n",
      "stderr": "",
      "error": null,
      "issue_counts": {
        "transaction_issues": 1
      }
    },
    "test_unused_features_documentation": {
      "status": "PASS",
      "stdout": "\n\ud83d\udcca PENSIEVE FEATURE UTILIZATION REPORT \ud83d\udcca\n\nCurrently UNUSED features that could enhance AutoTaskTracker:\n\n  \u274c REST API: Port 8839 API endpoints\n  \u274c Webhooks: Real-time screenshot events\n  \u274c Tagging: Entity tagging system\n  \u274c Export/Import: Data portability features\n  \u274c Multi-user: User management capabilities\n  \u274c Plugins: Pensieve plugin system\n  \u274c Advanced Search: Full-text search capabilities\n  \u274c Backup: Automated backup features\n\nConsider creating tickets to explore these capabilities.\nThis could significantly improve:\n- Real-time responsiveness (webhooks)\n- System integration (REST API)\n- Data organization (tagging)\n- Scalability (multi-user)\n\n",
      "stderr": "",
      "error": null,
      "issue_counts": {}
    }
  },
  "refactored": {
    "TestDatabaseHealth.test_bulk_operation_opportunities": {
      "status": "PASS",
      "stdout": "\n\ud83d\udca1 BULK OPERATION OPPORTUNITIES \ud83d\udca1\n\nFound 6 files with potential bulk operation improvements:\n\n  \ud83d\udca1 autotasktracker/pensieve/postgresql_adapter.py\n  \ud83d\udca1 autotasktracker/pensieve/vector_search.py\n  \ud83d\udca1 autotasktracker/pensieve/advanced_search.py\n  \ud83d\udca1 autotasktracker/pensieve/api_client.py\n  \ud83d\udca1 autotasktracker/core/database.py\n  \ud83d\udca1 autotasktracker/dashboards/realtime_dashboard.py\n\n\nConsider using bulk operations for better performance:\n\nInstead of:\n  for item in items:\n      db.store_metadata(entity_id, key, item)\n\nUse:\n  db.store_metadata_batch([(entity_id, key, item) for item in items])\n\n",
      "stderr": "",
      "error": null,
      "issue_counts": {
        "bulk_operations": 6
      }
    },
    "TestDatabaseHealth.test_connection_pool_usage": {
      "status": "PASS",
      "stdout": "\n\u26a0\ufe0f CONNECTION POOLING ISSUES \u26a0\ufe0f\n\nFound 2 potential connection pooling issues:\n\n\n\u26a0\ufe0f autotasktracker/pensieve/backend_optimizer.py - __init__()\n   Issue: Multiple DatabaseManager instances in function (3 instances)\n\n\n\u26a0\ufe0f autotasktracker/pensieve/enhanced_search.py - __init__()\n   Issue: Multiple DatabaseManager instances in function (3 instances)\n\n\n\u2705 BEST PRACTICES:\n\n1. Reuse DatabaseManager instances:\n   def __init__(self):\n       self.db = DatabaseManager()  # Create once\n   \n   def method1(self):\n       data = self.db.fetch_tasks()  # Reuse\n   \n   def method2(self):\n       data = self.db.fetch_tasks()  # Reuse same instance\n\n2. Use context managers for connections:\n   with self.db.get_connection() as conn:\n       # Connection automatically returned to pool\n\n3. For scripts, create once at module level:\n   db = DatabaseManager()  # Module level\n   \n   def main():\n       data = db.fetch_tasks()  # Use module instance\n\n",
      "stderr": "",
      "error": null,
      "issue_counts": {
        "connection_pooling": 2
      }
    },
    "TestDatabaseHealth.test_missing_index_usage": {
      "status": "PASS",
      "stdout": "",
      "stderr": "",
      "error": null,
      "issue_counts": {}
    },
    "TestDatabaseHealth.test_n_plus_one_query_patterns": {
      "status": "PASS",
      "stdout": "Processing files: 20/50\nProcessing files: 40/50\nProcessing files: 50/50\n\n\u26a0\ufe0f POTENTIAL N+1 QUERY PATTERNS DETECTED \u26a0\ufe0f\n\nFound 122 potential N+1 patterns in production code:\n\n\u274c autotasktracker/pensieve/event_processor.py:138\n   Loop at line 129 contains database query\n   Code: events = self._get_new_events()...\n\n\u274c autotasktracker/pensieve/event_processor.py:198\n   Loop at line 191 contains database query\n   Code: event = PensieveEvent(...\n\n\u274c autotasktracker/pensieve/event_processor.py:199\n   Loop at line 191 contains database query\n   Code: event_type=event_data.get('type', 'unknown'),...\n\n\u274c autotasktracker/pensieve/event_processor.py:200\n   Loop at line 191 contains database query\n   Code: entity_id=event_data.get('entity_id', 0),...\n\n\u274c autotasktracker/pensieve/event_processor.py:201\n   Loop at line 191 contains database query\n   Code: timestamp=datetime.fromisoformat(event_data.get('timestamp', datetime.now().isof...\n\n\u274c autotasktracker/pensieve/event_processor.py:201\n   Loop at line 191 contains database query\n   Code: timestamp=datetime.fromisoformat(event_data.get('timestamp', datetime.now().isof...\n\n\u274c autotasktracker/pensieve/event_processor.py:202\n   Loop at line 191 contains database query\n   Code: data=event_data.get('data', {}),...\n\n\u274c autotasktracker/pensieve/event_processor.py:517\n   Loop at line 515 contains database query\n   Code: stats = processor.get_statistics()...\n\n\u274c autotasktracker/pensieve/postgresql_adapter.py:147\n   Loop at line 141 contains database query\n   Code: metadata = self.pensieve_client.get_metadata(frame.id)...\n\n\u274c autotasktracker/pensieve/postgresql_adapter.py:154\n   Loop at line 141 contains database query\n   Code: \"tasks\": self._parse_tasks_safely(metadata.get(\"tasks\")),...\n\n... and 112 more\n\n\ud83d\udca1 Consider using batch operations or JOIN queries for better performance\n",
      "stderr": "",
      "error": null,
      "issue_counts": {
        "n_plus_one": 122
      }
    },
    "TestDatabaseHealth.test_no_direct_sqlite_access": {
      "status": "PASS",
      "stdout": "",
      "stderr": "",
      "error": null,
      "issue_counts": {}
    },
    "TestDatabaseHealth.test_transaction_management": {
      "status": "PASS",
      "stdout": "\n\u26a0\ufe0f TRANSACTION MANAGEMENT WARNING \u26a0\ufe0f\n\nFound 1 functions with multiple write operations but no transactions:\n\n\n\u26a0\ufe0f autotasktracker/pensieve/postgresql_adapter.py - get_migration_recommendations()\n   Line 362: 2 write operations without transaction\n   First operation at line 383: UPDATE\n\n\n\n\u2705 RECOMMENDED:\n  with db.get_connection() as conn:\n      cursor = conn.cursor()\n      cursor.execute(\"BEGIN TRANSACTION\")\n      try:\n          # Multiple operations\n          db.store_metadata(entity_id, \"tasks\", task_data, conn=conn)\n          db.store_metadata(entity_id, \"category\", category, conn=conn)\n          cursor.execute(\"COMMIT\")\n      except:\n          cursor.execute(\"ROLLBACK\")\n          raise\n\nThis ensures atomicity and better performance.\n\n",
      "stderr": "",
      "error": null,
      "issue_counts": {
        "transaction_issues": 1
      }
    },
    "TestIntegrationHealth.test_cache_management": {
      "status": "PASS",
      "stdout": "\n\u26a0\ufe0f CACHE MANAGEMENT WARNING \u26a0\ufe0f\n\nFound 2 files using cache without cleanup logic:\n\n  \u26a0\ufe0f autotasktracker/config.py\n  \u26a0\ufe0f autotasktracker/core/config_manager.py\n\nConsider implementing cache management:\n- Monitor cache size\n- Implement cleanup for old files\n- Set cache size limits\n- Add cache expiration\n\nExample:\n  def cleanup_cache(cache_dir, max_size_gb=10, max_age_days=30):\n      # Remove old files and limit cache size\n\n",
      "stderr": "",
      "error": null,
      "issue_counts": {
        "cache_management": 2
      }
    },
    "TestIntegrationHealth.test_generate_summary_report": {
      "status": "PASS",
      "stdout": "\n============================================================\nPENSIEVE HEALTH TEST SUMMARY\n============================================================\nMode: full\nFiles Analyzed: 50\nAuto-Fix: Disabled\n============================================================\n",
      "stderr": "",
      "error": null,
      "issue_counts": {}
    },
    "TestIntegrationHealth.test_memos_command_usage": {
      "status": "PASS",
      "stdout": "",
      "stderr": "",
      "error": null,
      "issue_counts": {}
    },
    "TestIntegrationHealth.test_metadata_key_consistency": {
      "status": "PASS",
      "stdout": "\n\ud83d\udea8 INCONSISTENT METADATA KEY USAGE DETECTED \ud83d\udea8\n\nFound 9 inconsistent metadata key patterns:\n\n\n\u274c Using 'ocr_result' instead of 'ocr_result':\n   Files: backend_optimizer.py, api_client.py, vector_search.py\n   ... and 7 more files\n\n\u274c Using 'text' instead of 'ocr_result':\n   Files: cache.py, vlm_monitor.py, advanced_analytics.py\n   ... and 3 more files\n\n\u274c Using 'active_window' instead of 'active_window':\n   Files: notifications.py, vlm_monitor.py, base.py\n   ... and 10 more files\n\n\u274c Using 'tasks' instead of 'tasks':\n   Files: visualizations.py, base.py, vector_search.py\n   ... and 7 more files\n\n\u274c Using 'extracted_tasks' instead of 'tasks':\n   Files: webhook_server.py, event_processor.py, realtime_dashboard.py\n   ... and 1 more files\n\n\u274c Using 'category' instead of 'category':\n   Files: notifications.py, visualizations.py, base.py\n   ... and 6 more files\n\n\u274c Using 'vlm_structured' instead of 'vlm_structured':\n   Files: pensieve_adapter.py, database.py\n\n\u274c Using 'window' instead of 'active_window':\n   Files: task_board.py\n\n\u274c Using 'vlm_description' instead of 'vlm_structured':\n   Files: vlm_monitor.py\n\n\ud83d\udca1 TIP: Set PENSIEVE_AUTO_FIX=1 to automatically fix metadata keys\n",
      "stderr": "",
      "error": null,
      "issue_counts": {
        "inconsistent_keys": 9
      }
    },
    "TestIntegrationHealth.test_pensieve_service_checks": {
      "status": "PASS",
      "stdout": "\n\u26a0\ufe0f MISSING PENSIEVE SERVICE CHECKS \u26a0\ufe0f\n\nFound 15 critical files without service checks:\n\n  \u26a0\ufe0f autotasktracker/dashboards/achievement_board.py\n  \u26a0\ufe0f autotasktracker/dashboards/cache.py\n  \u26a0\ufe0f autotasktracker/dashboards/__init__.py\n  \u26a0\ufe0f autotasktracker/dashboards/templates.py\n  \u26a0\ufe0f autotasktracker/dashboards/advanced_analytics.py\n  \u26a0\ufe0f autotasktracker/dashboards/utils.py\n  \u26a0\ufe0f autotasktracker/dashboards/realtime_dashboard.py\n  \u26a0\ufe0f autotasktracker/dashboards/analytics.py\n  \u26a0\ufe0f autotasktracker/dashboards/components/metrics.py\n  \u26a0\ufe0f autotasktracker/dashboards/components/data_display.py\n  \u26a0\ufe0f autotasktracker/dashboards/components/__init__.py\n  \u26a0\ufe0f autotasktracker/dashboards/components/visualizations.py\n  \u26a0\ufe0f autotasktracker/dashboards/components/filters.py\n  \u26a0\ufe0f autotasktracker/dashboards/data/models.py\n  \u26a0\ufe0f autotasktracker/dashboards/data/__init__.py\n\nAdd service status checks:\n\ndef check_pensieve_status():\n    try:\n        db = DatabaseManager()\n        if not db.test_connection():\n            st.error(\"Cannot connect to Pensieve database\")\n            st.info(\"Run: memos start\")\n            return False\n        return True\n    except Exception as e:\n        logger.error(\"Pensieve check failed in example code\")\n        return False\n\n",
      "stderr": "",
      "error": null,
      "issue_counts": {
        "missing_checks": 15
      }
    },
    "TestIntegrationHealth.test_rest_api_utilization": {
      "status": "PASS",
      "stdout": "\n\u2139\ufe0f REST API USAGE DETECTED (Progress!)\n\nFound REST API references in 3 files:\n  \u2705 autotasktracker/pensieve/config_reader.py\n  \u2705 autotasktracker/pensieve/config_sync.py\n  \u2705 autotasktracker/pensieve/api_client.py\n\nThis is good! The audit showed 0% REST API usage.\n\n",
      "stderr": "",
      "error": null,
      "issue_counts": {}
    },
    "TestIntegrationHealth.test_unused_features_documentation": {
      "status": "PASS",
      "stdout": "\n\ud83d\udcca PENSIEVE FEATURE UTILIZATION REPORT \ud83d\udcca\n\nCurrently UNUSED features that could enhance AutoTaskTracker:\n\n  \u274c REST API: Port 8839 API endpoints\n  \u274c Webhooks: Real-time screenshot events\n  \u274c Tagging: Entity tagging system\n  \u274c Export/Import: Data portability features\n  \u274c Multi-user: User management capabilities\n  \u274c Plugins: Pensieve plugin system\n  \u274c Advanced Search: Full-text search capabilities\n  \u274c Backup: Automated backup features\n\nConsider creating tickets to explore these capabilities.\nThis could significantly improve:\n- Real-time responsiveness (webhooks)\n- System integration (REST API)\n- Data organization (tagging)\n- Scalability (multi-user)\n\n",
      "stderr": "",
      "error": null,
      "issue_counts": {}
    },
    "TestErrorHealth.test_error_handling_patterns": {
      "status": "FAIL",
      "stdout": "Processing files: 20/50\nProcessing files: 40/50\nProcessing files: 50/50\n",
      "stderr": "",
      "error": "\n\ud83d\udea8 POOR ERROR HANDLING IN PENSIEVE INTEGRATION \ud83d\udea8\n\nFound 1 error handling issues in 1 files:\n\n\u274c autotasktracker/dashboards/base.py\n   Line 347: Silent Pass\n   Code: pass...\n\n\n\u2705 GOOD ERROR HANDLING:\n  try:\n      result = db.fetch_tasks()\n  except sqlite3.DatabaseError as e:\n      logger.error(f\"Database error: {{e}}\")\n      # Specific recovery action\n  except Exception as e:\n      logger.exception(\"Unexpected error\")\n      # Graceful degradation\n\n\u274c AVOID:\n  - Bare except clauses\n  - Printing errors instead of logging\n  - Silently passing exceptions\n  - Not handling specific error types\n\n\ud83d\udca1 TIP: Set PENSIEVE_AUTO_FIX=1 to automatically fix some issues\n",
      "issue_counts": {}
    },
    "TestErrorHealth.test_file_operation_validation": {
      "status": "FAIL",
      "stdout": "",
      "stderr": "",
      "error": "\n\ud83d\udea8 FILE OPERATION VALIDATION MISSING \ud83d\udea8\n\nFound 3 file operations without validation:\n\n\u274c autotasktracker/pensieve/config_sync.py:236\n   Function: _save_config_to_file()\n   Operation: file open\n   Code: open(self.config_file\n\n\n\u274c autotasktracker/core/config_manager.py:234\n   Function: save_config_snapshot()\n   Operation: file open\n   Code: open(file_path\n\n\n\u274c autotasktracker/dashboards/launcher.py:126\n   Function: launch_dashboard()\n   Operation: file open\n   Code: open(\n                cmd\n\n\n\u2705 ALWAYS VALIDATE FILE OPERATIONS:\n\n# Before reading:\nif not os.path.exists(filepath):\n    logger.error(f\"File not found: {{filepath}}\")\n    return None\n\n# Check permissions:\nif not os.access(filepath, os.R_OK):\n    raise PermissionError(f\"Cannot read file: {{filepath}}\")\n\n# Use Path for better validation:\nfrom pathlib import Path\nfile_path = Path(filepath)\nif not file_path.is_file():\n    raise FileNotFoundError(f\"Not a file: {{filepath}}\")\n\n# Always use try/except:\ntry:\n    with open(filepath, 'r') as f:\n        content = f.read()\nexcept IOError as e:\n    logger.error(f\"Failed to read file: {{e}}\")\n    raise\n",
      "issue_counts": {}
    },
    "TestErrorHealth.test_retry_logic_implementation": {
      "status": "PASS",
      "stdout": "\n\u26a0\ufe0f MISSING RETRY LOGIC WARNING \u26a0\ufe0f\n\nFound 6 files with network operations but no retry logic:\n\n\n\u26a0\ufe0f autotasktracker/pensieve/postgresql_adapter.py\n   Reason: Session requests without retry logic\n\n\n\u26a0\ufe0f autotasktracker/pensieve/event_integration.py\n   Reason: HTTP requests without retry logic\n\n\n\u26a0\ufe0f autotasktracker/core/database.py\n   Reason: Network connections without retry logic\n\n\n\u26a0\ufe0f autotasktracker/core/error_handler.py\n   Reason: HTTP requests without retry logic\n\n\n\u26a0\ufe0f autotasktracker/core/error_handler.py\n   Reason: Ollama API calls without retry logic\n\n\n\u26a0\ufe0f autotasktracker/dashboards/vlm_monitor.py\n   Reason: Ollama API calls without retry logic\n\n\n\n\u2705 IMPLEMENT RETRY LOGIC:\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10)\n)\ndef api_call_with_retry():\n    response = requests.get(url)\n    response.raise_for_status()\n    return response\n\nOr manual implementation:\nfor attempt in range(max_retries):\n    try:\n        result = api_call()\n        break\n    except Exception as e:\n        if attempt == max_retries - 1:\n            raise\n        time.sleep(2 ** attempt)  # Exponential backoff\n\n",
      "stderr": "",
      "error": null,
      "issue_counts": {
        "retry_issues": 6
      }
    },
    "TestConfigHealth.test_config_usage_patterns": {
      "status": "PASS",
      "stdout": "\n\ud83d\udcca CONFIGURATION USAGE ANALYSIS \ud83d\udcca\n\nFiles using configuration: 27\nGood configuration usage: 4\nFiles with no configuration: 23\n\nTop configuration users:\n\n\u2705 autotasktracker/config.py - Score: 3 (env vars, config module, defaults)\n\u2705 autotasktracker/pensieve/config_reader.py - Score: 3 (env vars, config module, defaults)\n\u2705 autotasktracker/pensieve/config_sync.py - Score: 3 (env vars, config module, defaults)\n\u2705 autotasktracker/core/config_manager.py - Score: 3 (env vars, config module, validation)\n\u2705 autotasktracker/__init__.py - Score: 1 (config module)\n\u2705 autotasktracker/pensieve/postgresql_adapter.py - Score: 1 (config module)\n\u2705 autotasktracker/pensieve/backend_optimizer.py - Score: 1 (config module)\n\u2705 autotasktracker/pensieve/__init__.py - Score: 1 (config module)\n\u2705 autotasktracker/pensieve/event_integration.py - Score: 1 (config module)\n\u2705 autotasktracker/pensieve/health_monitor.py - Score: 1 (config module)\n",
      "stderr": "",
      "error": null,
      "issue_counts": {}
    },
    "TestConfigHealth.test_configuration_hardcoding": {
      "status": "PASS",
      "stdout": "\n\u26a0\ufe0f HARDCODED VALUES DETECTED \u26a0\ufe0f\n\nFound 42 hardcoded values that should be configurable:\n\n\n\u26a0\ufe0f autotasktracker/pensieve/event_processor.py:123\n   Type: hardcoded timeout\n   Value: timeout=5\n   Context: self.processor_thread.join(timeout=5)...\n\n\n\u26a0\ufe0f autotasktracker/pensieve/event_processor.py:185\n   Type: hardcoded timeout\n   Value: timeout=1\n   Context: timeout=1,...\n\n\n\u26a0\ufe0f autotasktracker/pensieve/config_reader.py:63\n   Type: hardcoded timeout\n   Value: timeout=10\n   Context: timeout=10...\n\n\n\u26a0\ufe0f autotasktracker/pensieve/config_reader.py:283\n   Type: hardcoded URL\n   Value: \"http://{get_config().SERVER_HOST}:{pensieve_config.api_port}\"\n   Context: \"PENSIEVE_API_URL\": f\"http://{get_config().SERVER_HOST}:{pensieve_config.api_por...\n\n\n\u26a0\ufe0f autotasktracker/pensieve/config_reader.py:284\n   Type: hardcoded URL\n   Value: \"http://{get_config().SERVER_HOST}:{pensieve_config.web_port}\"\n   Context: \"PENSIEVE_WEB_URL\": f\"http://{get_config().SERVER_HOST}:{pensieve_config.web_por...\n\n\n\u26a0\ufe0f autotasktracker/pensieve/cache_manager.py:379\n   Type: hardcoded sleep\n   Value: sleep(300\n   Context: time.sleep(300)...\n\n\n\u26a0\ufe0f autotasktracker/pensieve/event_integration.py:244\n   Type: hardcoded timeout\n   Value: timeout=5\n   Context: requests.post(webhook_url, json=notification, timeout=5)...\n\n\n\u26a0\ufe0f autotasktracker/pensieve/event_integration.py:380\n   Type: hardcoded sleep\n   Value: sleep(5\n   Context: await asyncio.sleep(5)...\n\n\n\u26a0\ufe0f autotasktracker/pensieve/health_monitor.py:45\n   Type: hardcoded timeout\n   Value: timeout = 5\n   Context: self.api_timeout = 5...\n\n\n\u26a0\ufe0f autotasktracker/pensieve/health_monitor.py:210\n   Type: hardcoded timeout\n   Value: timeout=5\n   Context: self._monitor_thread.join(timeout=5)...\n\n\n\u26a0\ufe0f autotasktracker/pensieve/config_sync.py:26\n   Type: hardcoded port\n   Value: :8839\"\n   Context: api_base_url: str = \"http://localhost:8839\"...\n\n\n\u26a0\ufe0f autotasktracker/pensieve/config_sync.py:210\n   Type: hardcoded port\n   Value: :8839\"\n   Context: api_base_url=\"http://localhost:8839\"...\n\n\n\u26a0\ufe0f autotasktracker/pensieve/config_sync.py:142\n   Type: hardcoded URL\n   Value: \"http://{api_host}:{api_port}\"\n   Context: api_base_url = f\"http://{api_host}:{api_port}\"...\n\n\n\u26a0\ufe0f autotasktracker/pensieve/webhook_server.py:442\n   Type: hardcoded port\n   Value: :8840\"\n   Context: print(f\"Webhook server started on http://127.0.0.1:8840\")...\n\n\n\u26a0\ufe0f autotasktracker/pensieve/api_client.py:127\n   Type: hardcoded absolute path\n   Value: \"/api/libraries/{library_id}/folders/{folder_id}/entities\"\n   Context: endpoint=f\"/api/libraries/{library_id}/folders/{folder_id}/entities\"...\n\n... and 27 more\n\n\u2705 MAKE VALUES CONFIGURABLE:\n\n1. Move to configuration:\n   # config.py\n   VLM_TIMEOUT = int(os.getenv('VLM_TIMEOUT', '30'))\n   RETRY_ATTEMPTS = int(os.getenv('RETRY_ATTEMPTS', '3'))\n   BATCH_SIZE = int(os.getenv('BATCH_SIZE', '1000'))\n\n2. Use configuration:\n   from autotasktracker.config import get_config\n   config = get_config()\n   timeout = config.VLM_TIMEOUT\n\n3. For URLs:\n   base_url = config.API_BASE_URL\n   endpoint = f\"{base_url}/api/v1/process\"\n\nThis improves maintainability and deployment flexibility!\n\n",
      "stderr": "",
      "error": null,
      "issue_counts": {
        "hardcoded_values": 42
      }
    },
    "standalone.test_summary_report": {
      "status": "PASS",
      "stdout": "\n================================================================================\n                ENHANCED PENSIEVE INTEGRATION HEALTH CHECK COMPLETE\n================================================================================\n\nThis comprehensive health test now includes ALL improvements:\n\nPERFORMANCE ENHANCEMENTS:\n\u2713 Parallel execution (ProcessPoolExecutor)\n\u2713 Smart caching with file hashing\n\u2713 Incremental mode for CI/CD\n\u2713 Timeout protection per file\n\nAUTOMATION FEATURES:\n\u2713 Auto-fix metadata key consistency\n\u2713 Auto-fix error logging patterns\n\u2713 Configurable dry-run mode\n\u2713 CLI tool integration\n\nCORE INTEGRATION ISSUES:\n\u2713 Direct SQLite access detection\n\u2713 REST API utilization tracking\n\u2713 Metadata key consistency\n\u2713 Proper memos command usage\n\u2713 Transaction management\n\u2713 Service status checks\n\nPERFORMANCE PATTERNS:\n\u2713 N+1 query detection (AST-based)\n\u2713 Bulk operation opportunities\n\u2713 Database index optimization\n\u2713 Connection pool usage\n\nCODE QUALITY:\n\u2713 Error handling patterns\n\u2713 Retry logic implementation\n\u2713 File operation validation\n\u2713 Configuration hardcoding\n\nINFRASTRUCTURE:\n\u2713 Cache management\n\u2713 Feature utilization documentation\n\nCOVERAGE: 85-90% of audit findings with reduced false positives!\n\nUse: python scripts/pensieve_health_check.py [--fix] [--incremental]\n================================================================================\n\n",
      "stderr": "",
      "error": null,
      "issue_counts": {}
    }
  },
  "discrepancies": []
}