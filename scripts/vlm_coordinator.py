#!/usr/bin/env python3
"""
VLM Service Coordinator - Manages and coordinates multiple VLM services
"""
import sys
import os
import time
import json
import signal
import threading
import subprocess
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import logging

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from autotasktracker.core.database import DatabaseManager
from autotasktracker.core.error_handler import get_health_monitor, get_error_handler
from autotasktracker.ai.vlm_processor import SmartVLMProcessor

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ServiceRegistry:
    """Registry for VLM services."""
    
    def __init__(self):
        self.services = {}
        self.lock = threading.Lock()
        self.registry_file = Path.home() / '.memos' / 'vlm_services.json'
        self._load_registry()
    
    def register_service(self, service_id: str, service_info: Dict):
        """Register a VLM service."""
        with self.lock:
            self.services[service_id] = {
                **service_info,
                'registered_at': datetime.now().isoformat(),
                'last_heartbeat': datetime.now().isoformat(),
                'status': 'active'
            }
            self._save_registry()
            logger.info(f"Registered service: {service_id}")
    
    def unregister_service(self, service_id: str):
        """Unregister a VLM service."""
        with self.lock:
            if service_id in self.services:
                del self.services[service_id]
                self._save_registry()
                logger.info(f"Unregistered service: {service_id}")
    
    def heartbeat(self, service_id: str, stats: Dict = None):
        """Update service heartbeat."""
        with self.lock:
            if service_id in self.services:
                self.services[service_id]['last_heartbeat'] = datetime.now().isoformat()
                if stats:
                    self.services[service_id]['stats'] = stats
                self._save_registry()
    
    def get_active_services(self) -> Dict:
        """Get list of active services."""
        with self.lock:
            now = datetime.now()
            active = {}\n            \n            for service_id, info in self.services.items():\n                last_heartbeat = datetime.fromisoformat(info['last_heartbeat'])\n                if (now - last_heartbeat).total_seconds() < 300:  # 5 minute timeout\n                    active[service_id] = info\n                else:\n                    # Mark as inactive\n                    info['status'] = 'inactive'\n            \n            return active\n    \n    def _load_registry(self):\n        \"\"\"Load service registry from disk.\"\"\"\n        if self.registry_file.exists():\n            try:\n                with open(self.registry_file, 'r') as f:\n                    self.services = json.load(f)\n            except Exception as e:\n                logger.error(f\"Failed to load service registry: {e}\")\n                self.services = {}\n    \n    def _save_registry(self):\n        \"\"\"Save service registry to disk.\"\"\"\n        try:\n            self.registry_file.parent.mkdir(exist_ok=True)\n            with open(self.registry_file, 'w') as f:\n                json.dump(self.services, f, indent=2)\n        except Exception as e:\n            logger.error(f\"Failed to save service registry: {e}\")\n\n\nclass VLMCoordinator:\n    \"\"\"Coordinates multiple VLM services and manages workload distribution.\"\"\"\n    \n    def __init__(self):\n        self.registry = ServiceRegistry()\n        self.health_monitor = get_health_monitor()\n        self.error_handler = get_error_handler()\n        self.db = DatabaseManager()\n        self.running = False\n        \n        # Coordination settings\n        self.max_services = 3\n        self.target_queue_size = 50\n        self.scale_up_threshold = 100\n        self.scale_down_threshold = 10\n        \n        # Register health checks\n        self._register_health_checks()\n    \n    def _register_health_checks(self):\n        \"\"\"Register coordinator-specific health checks.\"\"\"\n        self.health_monitor.register_health_check(\n            'vlm_services_available', \n            self._check_services_available,\n            alert_threshold=2\n        )\n        \n        self.health_monitor.register_health_check(\n            'vlm_queue_size',\n            self._check_queue_size,\n            alert_threshold=3\n        )\n        \n        self.health_monitor.register_health_check(\n            'vlm_processing_rate',\n            self._check_processing_rate,\n            alert_threshold=5\n        )\n    \n    def _check_services_available(self) -> bool:\n        \"\"\"Check if VLM services are available.\"\"\"\n        active_services = self.registry.get_active_services()\n        return len(active_services) > 0\n    \n    def _check_queue_size(self) -> bool:\n        \"\"\"Check if VLM queue size is reasonable.\"\"\"\n        try:\n            pending_count = self._get_pending_vlm_count()\n            return pending_count < self.scale_up_threshold * 2\n        except Exception:\n            return False\n    \n    def _check_processing_rate(self) -> bool:\n        \"\"\"Check if VLM processing rate is adequate.\"\"\"\n        try:\n            # Check processing rate over last hour\n            query = \"\"\"\n            SELECT COUNT(*) as processed \n            FROM metadata_entries \n            WHERE key IN ('minicpm_v_result', 'vlm_structured')\n            AND created_at >= datetime('now', '-1 hour')\n            \"\"\"\n            \n            with self.db.get_connection() as conn:\n                cursor = conn.cursor()\n                cursor.execute(query)\n                result = cursor.fetchone()\n                processed_last_hour = result['processed'] if result else 0\n            \n            # Expect at least 10 processed per hour if there's a queue\n            pending_count = self._get_pending_vlm_count()\n            if pending_count > 50 and processed_last_hour < 10:\n                return False\n            \n            return True\n        except Exception:\n            return False\n    \n    def _get_pending_vlm_count(self) -> int:\n        \"\"\"Get count of pending VLM tasks.\"\"\"\n        query = \"\"\"\n        SELECT COUNT(*) as pending\n        FROM entities e\n        LEFT JOIN metadata_entries me_vlm \n            ON e.id = me_vlm.entity_id \n            AND me_vlm.key IN ('minicpm_v_result', 'vlm_structured')\n        WHERE e.file_type_group = 'image'\n        AND me_vlm.value IS NULL\n        AND e.created_at >= datetime('now', '-7 days')\n        \"\"\"\n        \n        try:\n            with self.db.get_connection() as conn:\n                cursor = conn.cursor()\n                cursor.execute(query)\n                result = cursor.fetchone()\n                return result['pending'] if result else 0\n        except Exception as e:\n            logger.error(f\"Error getting pending VLM count: {e}\")\n            return 0\n    \n    def start_service(self, service_type: str = 'processing', **kwargs) -> str:\n        \"\"\"Start a new VLM service.\"\"\"\n        service_id = f\"{service_type}_{int(time.time())}\"\n        \n        try:\n            if service_type == 'processing':\n                cmd = [\n                    sys.executable,\n                    'scripts/vlm_processing_service.py',\n                    '--workers', str(kwargs.get('workers', 2))\n                ]\n            elif service_type == 'optimizer':\n                cmd = [\n                    sys.executable,\n                    'scripts/vlm_batch_optimizer.py',\n                    '--batch-size', str(kwargs.get('batch_size', 20)),\n                    '--max-concurrent', str(kwargs.get('max_concurrent', 5))\n                ]\n            else:\n                raise ValueError(f\"Unknown service type: {service_type}\")\n            \n            # Start process\n            log_dir = Path.home() / '.memos' / 'logs'\n            log_dir.mkdir(exist_ok=True)\n            log_file = log_dir / f\"{service_id}.log\"\n            \n            with open(log_file, 'w') as f:\n                process = subprocess.Popen(\n                    cmd,\n                    stdout=f,\n                    stderr=subprocess.STDOUT,\n                    start_new_session=True\n                )\n            \n            # Register service\n            service_info = {\n                'type': service_type,\n                'pid': process.pid,\n                'cmd': ' '.join(cmd),\n                'log_file': str(log_file),\n                **kwargs\n            }\n            \n            self.registry.register_service(service_id, service_info)\n            logger.info(f\"Started {service_type} service: {service_id} (PID: {process.pid})\")\n            \n            return service_id\n            \n        except Exception as e:\n            logger.error(f\"Failed to start {service_type} service: {e}\")\n            raise\n    \n    def stop_service(self, service_id: str) -> bool:\n        \"\"\"Stop a VLM service.\"\"\"\n        try:\n            services = self.registry.get_active_services()\n            if service_id not in services:\n                logger.warning(f\"Service {service_id} not found or inactive\")\n                return False\n            \n            service_info = services[service_id]\n            pid = service_info.get('pid')\n            \n            if pid:\n                try:\n                    os.kill(pid, signal.SIGTERM)\n                    time.sleep(2)\n                    # Force kill if still running\n                    try:\n                        os.kill(pid, signal.SIGKILL)\n                    except ProcessLookupError:\n                        pass  # Process already terminated\n                except ProcessLookupError:\n                    pass  # Process already terminated\n            \n            self.registry.unregister_service(service_id)\n            logger.info(f\"Stopped service: {service_id}\")\n            return True\n            \n        except Exception as e:\n            logger.error(f\"Failed to stop service {service_id}: {e}\")\n            return False\n    \n    def auto_scale(self):\n        \"\"\"Automatically scale VLM services based on workload.\"\"\"\n        try:\n            pending_count = self._get_pending_vlm_count()\n            active_services = self.registry.get_active_services()\n            processing_services = [s for s in active_services.values() if s['type'] == 'processing']\n            \n            logger.info(f\"Auto-scale check: {pending_count} pending, {len(processing_services)} services\")\n            \n            # Scale up if needed\n            if (pending_count > self.scale_up_threshold and \n                len(processing_services) < self.max_services):\n                \n                logger.info(f\"Scaling up: {pending_count} pending tasks\")\n                self.start_service('processing', workers=2)\n            \n            # Scale down if needed\n            elif (pending_count < self.scale_down_threshold and \n                  len(processing_services) > 1):\n                \n                logger.info(f\"Scaling down: only {pending_count} pending tasks\")\n                # Stop the oldest service\n                oldest_service = min(\n                    processing_services,\n                    key=lambda s: s['registered_at']\n                )\n                for service_id, info in active_services.items():\n                    if info == oldest_service:\n                        self.stop_service(service_id)\n                        break\n        \n        except Exception as e:\n            logger.error(f\"Error in auto-scale: {e}\")\n    \n    def get_system_status(self) -> Dict:\n        \"\"\"Get comprehensive system status.\"\"\"\n        try:\n            active_services = self.registry.get_active_services()\n            pending_count = self._get_pending_vlm_count()\n            health_status = self.health_monitor.run_health_checks()\n            error_stats = self.error_handler.get_error_stats()\n            \n            # Get processing stats from a service\n            processor_stats = {}\n            if active_services:\n                try:\n                    processor = SmartVLMProcessor()\n                    processor_stats = processor.get_processing_stats()\n                except Exception:\n                    pass\n            \n            return {\n                'timestamp': datetime.now().isoformat(),\n                'services': {\n                    'active_count': len(active_services),\n                    'services': active_services\n                },\n                'workload': {\n                    'pending_count': pending_count,\n                    'scale_up_threshold': self.scale_up_threshold,\n                    'scale_down_threshold': self.scale_down_threshold\n                },\n                'health': health_status,\n                'errors': error_stats,\n                'processing': processor_stats\n            }\n        \n        except Exception as e:\n            logger.error(f\"Error getting system status: {e}\")\n            return {'error': str(e)}\n    \n    def run_coordinator(self, auto_scale: bool = True):\n        \"\"\"Run the coordinator main loop.\"\"\"\n        logger.info(\"Starting VLM Coordinator...\")\n        self.running = True\n        \n        # Setup signal handlers\n        signal.signal(signal.SIGINT, self._shutdown)\n        signal.signal(signal.SIGTERM, self._shutdown)\n        \n        try:\n            while self.running:\n                # Run health checks\n                health_status = self.health_monitor.run_health_checks()\n                \n                # Auto-scale if enabled\n                if auto_scale:\n                    self.auto_scale()\n                \n                # Log status\n                status = self.get_system_status()\n                logger.info(f\"Status: {status['services']['active_count']} services, \"\n                           f\"{status['workload']['pending_count']} pending\")\n                \n                # Wait before next cycle\n                time.sleep(60)  # Check every minute\n                \n        except KeyboardInterrupt:\n            self._shutdown()\n    \n    def _shutdown(self, signum=None, frame=None):\n        \"\"\"Graceful shutdown.\"\"\"\n        logger.info(\"Shutting down VLM Coordinator...\")\n        self.running = False\n        \n        # Stop all managed services\n        active_services = self.registry.get_active_services()\n        for service_id in active_services:\n            self.stop_service(service_id)\n        \n        sys.exit(0)\n\n\ndef main():\n    \"\"\"Main entry point.\"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='VLM Service Coordinator')\n    parser.add_argument('command', choices=['start', 'stop', 'status', 'scale'], \n                       help='Command to execute')\n    parser.add_argument('--service-id', help='Service ID for stop command')\n    parser.add_argument('--service-type', default='processing', \n                       choices=['processing', 'optimizer'],\n                       help='Type of service to start')\n    parser.add_argument('--workers', type=int, default=2,\n                       help='Number of workers for processing service')\n    parser.add_argument('--auto-scale', action='store_true',\n                       help='Enable auto-scaling')\n    parser.add_argument('--daemon', action='store_true',\n                       help='Run coordinator as daemon')\n    \n    args = parser.parse_args()\n    \n    coordinator = VLMCoordinator()\n    \n    if args.command == 'start':\n        if args.daemon:\n            # Run coordinator\n            coordinator.run_coordinator(auto_scale=args.auto_scale)\n        else:\n            # Start a service\n            service_id = coordinator.start_service(\n                args.service_type,\n                workers=args.workers\n            )\n            print(f\"Started service: {service_id}\")\n    \n    elif args.command == 'stop':\n        if args.service_id:\n            success = coordinator.stop_service(args.service_id)\n            print(f\"{'Stopped' if success else 'Failed to stop'} service: {args.service_id}\")\n        else:\n            print(\"Service ID required for stop command\")\n    \n    elif args.command == 'status':\n        status = coordinator.get_system_status()\n        print(json.dumps(status, indent=2))\n    \n    elif args.command == 'scale':\n        coordinator.auto_scale()\n        print(\"Auto-scale completed\")\n\n\nif __name__ == '__main__':\n    main()